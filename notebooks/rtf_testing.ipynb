{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T21:24:11.429542Z",
     "start_time": "2024-12-15T21:24:11.405449Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers.pytorch_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import transformers.pytorch_utils\n",
    "import src.data_processing as dp\n",
    "from sdmetrics.reports.single_table import QualityReport\n",
    "from sdmetrics.reports.single_table import DiagnosticReport\n",
    "from realtabformer import REaLTabFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "          ID  CT  UCSi  UCSh  Madh  SECS  BN  BC  NN  Mi  Class\n175  1001010   1     1     1     1     0   1   1   1   1      0\n162  1198611   3     1     1     1     0   1   3   1   1      0\n356   190561   1     3     0     1     3   1   0   1   1      0\n488  1065899   1     1     1     1     0   1   3   1   1      0\n409  1057938   3     1     1     1     0   1   1   1   1      0\n..       ...  ..   ...   ...   ...   ...  ..  ..  ..  ..    ...\n181  1006811  10     5     6    10     6  10   7   7  10      1\n448  1080058   1     1     1     1     0   1   1   0   1      0\n112  1173035   3     3     0     1     0   3   3   1   1      0\n557   183936   3     1     1     1     0   1   0   1   1      0\n58   1115080   5     3     5     5     3   3   1  10   1      1\n\n[137 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>CT</th>\n      <th>UCSi</th>\n      <th>UCSh</th>\n      <th>Madh</th>\n      <th>SECS</th>\n      <th>BN</th>\n      <th>BC</th>\n      <th>NN</th>\n      <th>Mi</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>175</th>\n      <td>1001010</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>1198611</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>190561</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>488</th>\n      <td>1065899</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>409</th>\n      <td>1057938</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>1006811</td>\n      <td>10</td>\n      <td>5</td>\n      <td>6</td>\n      <td>10</td>\n      <td>6</td>\n      <td>10</td>\n      <td>7</td>\n      <td>7</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>448</th>\n      <td>1080058</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>1173035</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>557</th>\n      <td>183936</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>1115080</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>137 rows Ã— 11 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data, sample_data = dp.csv_data_split(\"../data/breast-cancer-wisconsin.csv\")\n",
    "my_metadata_dict = dp.metadata(\"../data/cancer_metadata.json\")\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T21:07:37.776815Z",
     "start_time": "2024-12-15T21:07:37.740468Z"
    }
   },
   "id": "c2be796ff8fab988"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "model = REaLTabFormer.load_from_dir(\"../models/rtf_small/id000017342868701547638784\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T22:20:58.395125Z",
     "start_time": "2024-12-15T22:20:56.596097Z"
    }
   },
   "id": "7195c1b95d6331d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synthetic_data = model.sample(n_samples=len(test_data))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d94259ded72a67cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quality = QualityReport()\n",
    "quality.generate(test_data,synthetic_data,my_metadata_dict,verbose=False)\n",
    "quality.get_properties()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf77f9297b8d5ca9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diagnostic = DiagnosticReport()\n",
    "diagnostic.generate(test_data,synthetic_data,my_metadata_dict,verbose=False)\n",
    "diagnostic.get_properties()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68c5912caac7d3c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"small_model\": \"../models/rtf_small/id000017342868701547638784\",\n",
    "    \"regular_model\": \"../models/rtf_regular/id000017342890144858071040\",\n",
    "    # \"large_model\": \"../models/rtf_large/id000017341472610579369984\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "490b08f4fffcffba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae86cedafc67cbdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_generations = 5\n",
    "\n",
    "# Loop through different models\n",
    "for model_name, model_path in models_dict.items():\n",
    "    # Load the model\n",
    "    model = REaLTabFormer.load_from_dir(model_path)\n",
    "    \n",
    "    # Initialize accumulators for scores\n",
    "    column_shapes_scores = []\n",
    "    column_pair_trends_scores = []\n",
    "    data_validity_scores = []\n",
    "    data_structure_scores = []\n",
    "    \n",
    "    # Generate multiple synthetic datasets and compute scores\n",
    "    for _ in range(n_generations):\n",
    "        synthetic_data = model.sample(n_samples=len(test_data))\n",
    "        \n",
    "        # Generate quality and diagnostic reports\n",
    "        quality = QualityReport()\n",
    "        quality.generate(test_data, synthetic_data, my_metadata_dict, verbose=False)\n",
    "        diagnostic = DiagnosticReport()\n",
    "        diagnostic.generate(test_data, synthetic_data, my_metadata_dict, verbose=False)\n",
    "        \n",
    "        # Extract individual scores\n",
    "        column_shapes = quality.get_properties().loc[\n",
    "            quality.get_properties()['Property'] == 'Column Shapes', 'Score'\n",
    "        ].values[0]\n",
    "        column_pair_trends = quality.get_properties().loc[\n",
    "            quality.get_properties()['Property'] == 'Column Pair Trends', 'Score'\n",
    "        ].values[0]\n",
    "        data_validity = diagnostic.get_properties().loc[\n",
    "            diagnostic.get_properties()['Property'] == 'Data Validity', 'Score'\n",
    "        ].values[0]\n",
    "        data_structure = diagnostic.get_properties().loc[\n",
    "            diagnostic.get_properties()['Property'] == 'Data Structure', 'Score'\n",
    "        ].values[0]\n",
    "        \n",
    "        # Append scores to accumulators\n",
    "        column_shapes_scores.append(column_shapes)\n",
    "        column_pair_trends_scores.append(column_pair_trends)\n",
    "        data_validity_scores.append(data_validity)\n",
    "        data_structure_scores.append(data_structure)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_column_shapes = sum(column_shapes_scores) / n_generations\n",
    "    avg_column_pair_trends = sum(column_pair_trends_scores) / n_generations\n",
    "    avg_data_validity = sum(data_validity_scores) / n_generations\n",
    "    avg_data_structure = sum(data_structure_scores) / n_generations\n",
    "    \n",
    "    # Calculate total score\n",
    "    avg_total_score = (\n",
    "        0.40 * avg_column_shapes +\n",
    "        0.40 * avg_column_pair_trends +\n",
    "        0.10 * avg_data_validity +\n",
    "        0.10 * avg_data_structure\n",
    "    )\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Avg Column Shapes\": avg_column_shapes,\n",
    "        \"Avg Column Pair Trends\": avg_column_pair_trends,\n",
    "        \"Avg Data Validity\": avg_data_validity,\n",
    "        \"Avg Data Structure\": avg_data_structure,\n",
    "        \"Avg Total Score\": avg_total_score\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7c9959e15f04ce0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp = train_data.head()\n",
    "temp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "647282c1dfab5e25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "    data=test_data.drop('Class', axis=1),\n",
    "    target_col='Class',\n",
    "    disable_progress_bar = True,\n",
    "    fillunk=False,\n",
    "    target_pos_val=1\n",
    ")\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "967ab7e3fa561e18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data['Class'].head(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2275946c4a70e4d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sdmetrics.single_table import BinaryDecisionTreeClassifier\n",
    "\n",
    "def load_and_generate_synthetic_data(model_dir, n_samples, metadata):\n",
    "    model = REaLTabFormer.load_from_dir(model_dir)\n",
    "    synthetic_data = model.sample(n_samples=n_samples)\n",
    "    return synthetic_data\n",
    "\n",
    "def evaluate_model(test_data, synthetic_data, target, metadata):\n",
    "    return BinaryDecisionTreeClassifier.compute(\n",
    "        test_data=test_data,\n",
    "        train_data=synthetic_data,\n",
    "        target=target,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "# Model directories\n",
    "model_dirs = [\n",
    "    \"../models/rtf_small/id000017342868701547638784\",\n",
    "    \"../models/rtf_regular/id000017342890144858071040\",\n",
    "    \"../models/rtf_large/id000017342929846661560320\"\n",
    "]\n",
    "\n",
    "# Number of runs\n",
    "n_runs = 5\n",
    "\n",
    "# Evaluate each model\n",
    "for model_dir in model_dirs:\n",
    "    scores = []\n",
    "    for _ in range(n_runs):\n",
    "        # Generate synthetic data\n",
    "        synthetic_data = load_and_generate_synthetic_data(model_dir, len(test_data), my_metadata_dict)\n",
    "        \n",
    "        # Evaluate the synthetic data\n",
    "        evaluation_score = evaluate_model(test_data, synthetic_data, target='Class', metadata=my_metadata_dict)\n",
    "        \n",
    "        # Append score to list\n",
    "        scores.append(evaluation_score)\n",
    "\n",
    "    # Compute average score\n",
    "    average_score = np.mean(scores)\n",
    "    print(f\"Average Evaluation for model {model_dir}: {average_score}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "831ae2fb5fc1e6d4"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "def prune_conv1d_layer(layer,amount):\n",
    "    prune.ln_structured(layer, name='weight', amount=amount, dim=1,n=float('-inf'))\n",
    "    prune.remove(layer,name='weight')\n",
    "    \n",
    "def apply_structured_pruning(model,amount):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module):\n",
    "            if isinstance(module, transformers.pytorch_utils.Conv1D):\n",
    "                prune_conv1d_layer(module,amount)\n",
    "                \n",
    "def print_tensor(model):\n",
    "    sparse_model = model\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() == 2:\n",
    "            print(name,param.data.to_sparse())\n",
    "\n",
    "def convert_to_sparse(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() == 2:  # Apply to weight matrices\n",
    "            # Convert to sparse tensor\n",
    "            param.data  = param.data.to_sparse()\n",
    "        \n",
    "            # Replace the parameter data with the sparse tensor\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:59:47.358469Z",
     "start_time": "2024-12-16T01:59:47.336232Z"
    }
   },
   "id": "7861a9a6e790d0c9"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight tensor(indices=tensor([[  0,   0,   0,  ..., 155, 155, 155],\n",
      "                       [  0,   1,   2,  ..., 509, 510, 511]]),\n",
      "       values=tensor([-0.0103, -0.0030,  0.0100,  ..., -0.0320, -0.0024,\n",
      "                      -0.0076]),\n",
      "       size=(156, 512), nnz=79872, layout=torch.sparse_coo)\n",
      "transformer.wpe.weight tensor(indices=tensor([[   0,    0,    0,  ..., 1023, 1023, 1023],\n",
      "                       [   0,    1,    2,  ...,  509,  510,  511]]),\n",
      "       values=tensor([ 0.0077,  0.0175,  0.0051,  ..., -0.0079, -0.0042,\n",
      "                       0.0200]),\n",
      "       size=(1024, 512), nnz=524288, layout=torch.sparse_coo)\n",
      "transformer.h.0.attn.c_attn.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 1533, 1534, 1535]]),\n",
      "       values=tensor([-8.1597e-04,  4.7900e-03, -2.7935e-02,  ...,\n",
      "                       3.9678e-02, -2.9065e-02,  8.4840e-05]),\n",
      "       size=(512, 1536), nnz=786432, layout=torch.sparse_coo)\n",
      "transformer.h.0.attn.c_proj.weight tensor(indices=tensor([[  0,   0,   0,  ..., 511, 511, 511],\n",
      "                       [  0,   1,   2,  ..., 509, 510, 511]]),\n",
      "       values=tensor([-0.0010,  0.0015, -0.0020,  ...,  0.0096,  0.0013,\n",
      "                       0.0046]),\n",
      "       size=(512, 512), nnz=262144, layout=torch.sparse_coo)\n",
      "transformer.h.0.mlp.c_fc.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 2045, 2046, 2047]]),\n",
      "       values=tensor([ 0.0071,  0.0039,  0.0103,  ..., -0.0125, -0.0253,\n",
      "                      -0.0290]),\n",
      "       size=(512, 2048), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.0.mlp.c_proj.weight tensor(indices=tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
      "                       [   0,    1,    2,  ...,  509,  510,  511]]),\n",
      "       values=tensor([-0.0042, -0.0047, -0.0043,  ...,  0.0075,  0.0047,\n",
      "                       0.0087]),\n",
      "       size=(2048, 512), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.1.attn.c_attn.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 1533, 1534, 1535]]),\n",
      "       values=tensor([ 0.0177,  0.0022, -0.0037,  ..., -0.0220,  0.0292,\n",
      "                       0.0153]),\n",
      "       size=(512, 1536), nnz=786432, layout=torch.sparse_coo)\n",
      "transformer.h.1.attn.c_proj.weight tensor(indices=tensor([[  0,   0,   0,  ..., 511, 511, 511],\n",
      "                       [  0,   1,   2,  ..., 509, 510, 511]]),\n",
      "       values=tensor([ 2.5473e-03,  3.1782e-05,  2.5045e-03,  ...,\n",
      "                      -6.7930e-03, -5.3440e-03, -2.5619e-03]),\n",
      "       size=(512, 512), nnz=262144, layout=torch.sparse_coo)\n",
      "transformer.h.1.mlp.c_fc.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 2045, 2046, 2047]]),\n",
      "       values=tensor([-0.0025,  0.0169,  0.0127,  ..., -0.0216, -0.0214,\n",
      "                       0.0094]),\n",
      "       size=(512, 2048), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.1.mlp.c_proj.weight tensor(indices=tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
      "                       [   0,    1,    2,  ...,  509,  510,  511]]),\n",
      "       values=tensor([ 0.0087, -0.0050,  0.0035,  ...,  0.0098, -0.0079,\n",
      "                      -0.0120]),\n",
      "       size=(2048, 512), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.2.attn.c_attn.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 1533, 1534, 1535]]),\n",
      "       values=tensor([-0.0044,  0.0435,  0.0051,  ..., -0.0276, -0.0076,\n",
      "                      -0.0179]),\n",
      "       size=(512, 1536), nnz=786432, layout=torch.sparse_coo)\n",
      "transformer.h.2.attn.c_proj.weight tensor(indices=tensor([[  0,   0,   0,  ..., 511, 511, 511],\n",
      "                       [  0,   1,   2,  ..., 509, 510, 511]]),\n",
      "       values=tensor([ 0.0049, -0.0019,  0.0075,  ..., -0.0011,  0.0026,\n",
      "                       0.0017]),\n",
      "       size=(512, 512), nnz=262144, layout=torch.sparse_coo)\n",
      "transformer.h.2.mlp.c_fc.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 2045, 2046, 2047]]),\n",
      "       values=tensor([-0.0075,  0.0237,  0.0002,  ...,  0.0299,  0.0507,\n",
      "                       0.0017]),\n",
      "       size=(512, 2048), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.2.mlp.c_proj.weight tensor(indices=tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
      "                       [   0,    1,    2,  ...,  509,  510,  511]]),\n",
      "       values=tensor([ 0.0020,  0.0047,  0.0069,  ..., -0.0013, -0.0104,\n",
      "                      -0.0047]),\n",
      "       size=(2048, 512), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.3.attn.c_attn.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 1533, 1534, 1535]]),\n",
      "       values=tensor([-0.0030,  0.0009, -0.0091,  ..., -0.0066,  0.0172,\n",
      "                       0.0101]),\n",
      "       size=(512, 1536), nnz=786432, layout=torch.sparse_coo)\n",
      "transformer.h.3.attn.c_proj.weight tensor(indices=tensor([[  0,   0,   0,  ..., 511, 511, 511],\n",
      "                       [  0,   1,   2,  ..., 509, 510, 511]]),\n",
      "       values=tensor([-0.0123, -0.0018, -0.0031,  ..., -0.0018, -0.0142,\n",
      "                       0.0051]),\n",
      "       size=(512, 512), nnz=262144, layout=torch.sparse_coo)\n",
      "transformer.h.3.mlp.c_fc.weight tensor(indices=tensor([[   0,    0,    0,  ...,  511,  511,  511],\n",
      "                       [   0,    1,    2,  ..., 2045, 2046, 2047]]),\n",
      "       values=tensor([-0.0032,  0.0124,  0.0050,  ..., -0.0045,  0.0104,\n",
      "                       0.0119]),\n",
      "       size=(512, 2048), nnz=1048576, layout=torch.sparse_coo)\n",
      "transformer.h.3.mlp.c_proj.weight tensor(indices=tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
      "                       [   0,    1,    2,  ...,  509,  510,  511]]),\n",
      "       values=tensor([-0.0029,  0.0022,  0.0159,  ...,  0.0004, -0.0022,\n",
      "                       0.0050]),\n",
      "       size=(2048, 512), nnz=1048576, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "print_tensor(model.model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:59:49.374050Z",
     "start_time": "2024-12-16T01:59:49.158423Z"
    }
   },
   "id": "7eba6c62e04d3087"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.named_modules():        \n",
    "    if isinstance(module, transformers.pytorch_utils.Conv1D):\n",
    "            print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:59:50.505616Z",
     "start_time": "2024-12-16T01:59:50.484222Z"
    }
   },
   "id": "d67297ae05891bc2"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.00%\n",
      "Total: 13214720\n",
      "Zero: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "        zero_params += (param == 0).sum().item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity,total_params, zero_params\n",
    "\n",
    "# Example usage\n",
    "sparsity, total_params, zero_params = compute_sparsity(model.model)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n",
    "print(f\"Total: {total_params}\")\n",
    "print(f\"Zero: {zero_params}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T21:36:34.315894Z",
     "start_time": "2024-12-15T21:36:34.260616Z"
    }
   },
   "id": "f002c9f686256399"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying artefacts from: best-disc-model\n",
      "Copying artefacts from: mean-best-disc-model\n",
      "Copying artefacts from: not-best-disc-model\n",
      "Copying artefacts from: last-epoch-model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/small/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:49.854386Z",
     "start_time": "2024-12-15T21:21:49.489107Z"
    }
   },
   "id": "13c356210b8649f6"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "def apply_quantization_to_conv1d(model):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Apply dynamic quantization to Conv1D layers in the model\n",
    "    quantized_model = quantize_dynamic(\n",
    "        model,  # The model to quantize\n",
    "        dtype=torch.qint8  # Use int8 for more space reduction\n",
    "    )\n",
    "\n",
    "\n",
    "    return quantized_model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T22:08:38.669338Z",
     "start_time": "2024-12-15T22:08:38.651098Z"
    }
   },
   "id": "91188d38314db5f6"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attribute 'dtype' of 'torch._C.TensorBase' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[99], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m quantized_model \u001B[38;5;241m=\u001B[39m apply_quantization_to_conv1d(model\u001B[38;5;241m.\u001B[39mmodel)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, param \u001B[38;5;129;01min\u001B[39;00m quantized_model\u001B[38;5;241m.\u001B[39mnamed_parameters():\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mqint8\n",
      "\u001B[0;31mAttributeError\u001B[0m: attribute 'dtype' of 'torch._C.TensorBase' objects is not writable"
     ]
    }
   ],
   "source": [
    "quantized_model = apply_quantization_to_conv1d(model.model)\n",
    "for name, param in quantized_model.named_parameters():\n",
    "    param.dtype = torch.qint8\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T22:09:24.726217Z",
     "start_time": "2024-12-15T22:09:24.647504Z"
    }
   },
   "id": "8ae7e17640a5363a"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), 'quantized_model.pt')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T21:57:50.938762Z",
     "start_time": "2024-12-15T21:57:50.491793Z"
    }
   },
   "id": "d793dc8d20b914d5"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight Parameter containing:\n",
      "tensor([[-0.0103, -0.0030,  0.0100,  ...,  0.0099,  0.0353,  0.0271],\n",
      "        [-0.0331, -0.0066,  0.0276,  ..., -0.0197,  0.0526,  0.0090],\n",
      "        [-0.0060,  0.0167, -0.0007,  ..., -0.0411,  0.0145, -0.0070],\n",
      "        ...,\n",
      "        [-0.0329,  0.0221,  0.0433,  ...,  0.0092,  0.0271, -0.0329],\n",
      "        [ 0.0361, -0.0400, -0.0088,  ...,  0.0068, -0.0069, -0.0390],\n",
      "        [ 0.0380,  0.0129,  0.0190,  ..., -0.0320, -0.0024, -0.0076]],\n",
      "       requires_grad=True)\n",
      "transformer.wpe.weight Parameter containing:\n",
      "tensor([[ 0.0077,  0.0175,  0.0051,  ..., -0.0035, -0.0094, -0.0087],\n",
      "        [-0.0262,  0.0033, -0.0102,  ..., -0.0058, -0.0033,  0.0025],\n",
      "        [ 0.0009, -0.0046, -0.0098,  ..., -0.0049,  0.0086,  0.0193],\n",
      "        ...,\n",
      "        [ 0.0089,  0.0011, -0.0070,  ..., -0.0304,  0.0046, -0.0209],\n",
      "        [ 0.0211, -0.0381, -0.0137,  ...,  0.0087,  0.0621, -0.0023],\n",
      "        [-0.0184,  0.0072,  0.0381,  ..., -0.0079, -0.0042,  0.0200]],\n",
      "       requires_grad=True)\n",
      "transformer.h.0.attn.c_attn.weight Parameter containing:\n",
      "tensor([[-0., 0., -0.,  ..., 0., 0., -0.],\n",
      "        [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [-0., 0., -0.,  ..., -0., 0., -0.],\n",
      "        [-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., -0.,  ..., 0., -0., 0.]], requires_grad=True)\n",
      "transformer.h.0.attn.c_proj.weight Parameter containing:\n",
      "tensor([[-0.0000,  0.0000, -0.0000,  ..., -0.0009,  0.0092,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0053,  0.0061, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0039, -0.0002, -0.0000],\n",
      "        ...,\n",
      "        [-0.0000,  0.0000, -0.0000,  ...,  0.0074,  0.0039, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0003,  0.0212,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  ...,  0.0096,  0.0013,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.0.mlp.c_fc.weight Parameter containing:\n",
      "tensor([[ 0.0071,  0.0000,  0.0103,  ...,  0.0110,  0.0121,  0.0272],\n",
      "        [ 0.0213,  0.0000,  0.0385,  ...,  0.0303, -0.0047,  0.0258],\n",
      "        [ 0.0063,  0.0000,  0.0047,  ..., -0.0112, -0.0439, -0.0005],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0000, -0.0103,  ..., -0.0178,  0.0106,  0.0224],\n",
      "        [-0.0105,  0.0000,  0.0388,  ...,  0.0156,  0.0144,  0.0123],\n",
      "        [ 0.0064,  0.0000, -0.0177,  ..., -0.0125, -0.0253, -0.0290]],\n",
      "       requires_grad=True)\n",
      "transformer.h.0.mlp.c_proj.weight Parameter containing:\n",
      "tensor([[-0.0042, -0.0047, -0.0000,  ...,  0.0000, -0.0061, -0.0000],\n",
      "        [ 0.0040, -0.0114, -0.0000,  ...,  0.0000, -0.0007, -0.0000],\n",
      "        [-0.0028, -0.0127,  0.0000,  ..., -0.0000,  0.0109, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0005,  0.0037, -0.0000,  ..., -0.0000,  0.0080,  0.0000],\n",
      "        [ 0.0035, -0.0052,  0.0000,  ..., -0.0000,  0.0072,  0.0000],\n",
      "        [ 0.0065, -0.0107,  0.0000,  ...,  0.0000,  0.0047,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.1.attn.c_attn.weight Parameter containing:\n",
      "tensor([[ 0.0177,  0.0022, -0.0000,  ...,  0.0425,  0.0325,  0.0000],\n",
      "        [-0.0018, -0.0373, -0.0000,  ...,  0.0120,  0.0059, -0.0000],\n",
      "        [-0.0047, -0.0028, -0.0000,  ...,  0.0148, -0.0028, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0053,  0.0081, -0.0000,  ..., -0.0178,  0.0166,  0.0000],\n",
      "        [-0.0032, -0.0202,  0.0000,  ..., -0.0006, -0.0065, -0.0000],\n",
      "        [ 0.0164,  0.0155,  0.0000,  ..., -0.0220,  0.0292,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.1.attn.c_proj.weight Parameter containing:\n",
      "tensor([[ 0.0000e+00,  3.1782e-05,  2.5045e-03,  ...,  5.3348e-03,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00, -5.9168e-03, -6.7721e-03,  ..., -2.7816e-03,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  1.3514e-03, -3.5897e-03,  ...,  8.1503e-04,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.7479e-02,  3.5242e-03,  ..., -1.8146e-04,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -8.7000e-03,  3.8022e-03,  ...,  4.8304e-03,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00,  1.1461e-04, -3.3508e-03,  ..., -6.7930e-03,\n",
      "         -0.0000e+00, -0.0000e+00]], requires_grad=True)\n",
      "transformer.h.1.mlp.c_fc.weight Parameter containing:\n",
      "tensor([[-0.0025,  0.0000,  0.0000,  ...,  0.0000,  0.0176, -0.0000],\n",
      "        [ 0.0087, -0.0000, -0.0000,  ..., -0.0000,  0.0067,  0.0000],\n",
      "        [ 0.0344,  0.0000,  0.0000,  ..., -0.0000,  0.0068,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0000,  0.0000,  ...,  0.0000, -0.0019,  0.0000],\n",
      "        [-0.0469,  0.0000,  0.0000,  ...,  0.0000,  0.0188,  0.0000],\n",
      "        [ 0.0079, -0.0000,  0.0000,  ..., -0.0000, -0.0214,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.1.mlp.c_proj.weight Parameter containing:\n",
      "tensor([[ 0.0000, -0.0000,  0.0000,  ...,  0.0148,  0.0026,  0.0031],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0065,  0.0140, -0.0054],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0009,  0.0049,  0.0011],\n",
      "        ...,\n",
      "        [-0.0000, -0.0000, -0.0000,  ...,  0.0019,  0.0140,  0.0054],\n",
      "        [ 0.0000, -0.0000,  0.0000,  ..., -0.0028, -0.0010, -0.0108],\n",
      "        [ 0.0000, -0.0000,  0.0000,  ...,  0.0098, -0.0079, -0.0120]],\n",
      "       requires_grad=True)\n",
      "transformer.h.2.attn.c_attn.weight Parameter containing:\n",
      "tensor([[-0.0044,  0.0435,  0.0000,  ..., -0.0053, -0.0609,  0.0123],\n",
      "        [ 0.0140, -0.0085, -0.0000,  ...,  0.0102, -0.0041,  0.0084],\n",
      "        [ 0.0107,  0.0073,  0.0000,  ..., -0.0082, -0.0082,  0.0274],\n",
      "        ...,\n",
      "        [-0.0090, -0.0416,  0.0000,  ..., -0.0422,  0.0190, -0.0170],\n",
      "        [-0.0143, -0.0061,  0.0000,  ...,  0.0097,  0.0016, -0.0035],\n",
      "        [-0.0258,  0.0052,  0.0000,  ..., -0.0276, -0.0076, -0.0179]],\n",
      "       requires_grad=True)\n",
      "transformer.h.2.attn.c_proj.weight Parameter containing:\n",
      "tensor([[ 0.0000, -0.0019,  0.0000,  ..., -0.0000,  0.0076,  0.0000],\n",
      "        [-0.0000,  0.0143, -0.0000,  ..., -0.0000,  0.0065, -0.0000],\n",
      "        [ 0.0000,  0.0053,  0.0000,  ...,  0.0000,  0.0001,  0.0000],\n",
      "        ...,\n",
      "        [-0.0000, -0.0087, -0.0000,  ..., -0.0000,  0.0055,  0.0000],\n",
      "        [ 0.0000,  0.0052,  0.0000,  ...,  0.0000, -0.0086,  0.0000],\n",
      "        [-0.0000,  0.0039,  0.0000,  ..., -0.0000,  0.0026,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.2.mlp.c_fc.weight Parameter containing:\n",
      "tensor([[-7.4921e-03,  2.3724e-02,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          6.8488e-03,  1.3514e-02],\n",
      "        [ 1.8680e-02, -1.7768e-02, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -5.7893e-03, -3.0495e-02],\n",
      "        [ 4.2923e-03, -2.5142e-02, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -2.3301e-02,  1.3124e-02],\n",
      "        ...,\n",
      "        [-2.0747e-02,  8.4044e-05, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          1.9295e-02, -4.7273e-03],\n",
      "        [-3.1391e-03, -1.1982e-02,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -3.4314e-02, -1.2524e-03],\n",
      "        [ 3.6331e-02,  3.7975e-02, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          5.0673e-02,  1.6528e-03]], requires_grad=True)\n",
      "transformer.h.2.mlp.c_proj.weight Parameter containing:\n",
      "tensor([[ 0.0020,  0.0047,  0.0069,  ...,  0.0082, -0.0000, -0.0056],\n",
      "        [ 0.0024,  0.0016,  0.0032,  ...,  0.0056, -0.0000, -0.0019],\n",
      "        [-0.0014, -0.0060,  0.0073,  ...,  0.0039, -0.0000,  0.0012],\n",
      "        ...,\n",
      "        [-0.0132, -0.0039,  0.0146,  ...,  0.0038, -0.0000,  0.0028],\n",
      "        [-0.0013, -0.0030,  0.0028,  ...,  0.0093,  0.0000, -0.0048],\n",
      "        [-0.0008,  0.0042,  0.0093,  ..., -0.0013, -0.0000, -0.0047]],\n",
      "       requires_grad=True)\n",
      "transformer.h.3.attn.c_attn.weight Parameter containing:\n",
      "tensor([[-0.0000,  0.0009, -0.0000,  ..., -0.0006,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0114,  0.0000,  ..., -0.0063,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0017, -0.0000,  ...,  0.0214,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0172, -0.0000,  ..., -0.0033,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0070,  0.0000,  ...,  0.0225, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0010, -0.0000,  ..., -0.0066,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.3.attn.c_proj.weight Parameter containing:\n",
      "tensor([[-0.0000, -0.0000, -0.0031,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0039,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0021,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0115,  ..., -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0047,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0041,  ..., -0.0000, -0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "transformer.h.3.mlp.c_fc.weight Parameter containing:\n",
      "tensor([[-0.0032,  0.0124,  0.0050,  ...,  0.0000, -0.0000,  0.0444],\n",
      "        [ 0.0099,  0.0197, -0.0166,  ...,  0.0000, -0.0000, -0.0356],\n",
      "        [ 0.0118, -0.0098, -0.0283,  ...,  0.0000, -0.0000,  0.0021],\n",
      "        ...,\n",
      "        [-0.0073, -0.0136, -0.0148,  ...,  0.0000, -0.0000, -0.0018],\n",
      "        [-0.0141, -0.0228, -0.0126,  ...,  0.0000,  0.0000,  0.0246],\n",
      "        [ 0.0019, -0.0047, -0.0070,  ..., -0.0000,  0.0000,  0.0119]],\n",
      "       requires_grad=True)\n",
      "transformer.h.3.mlp.c_proj.weight Parameter containing:\n",
      "tensor([[-0.0029,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0034,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0081,  0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0085, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0035, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0075, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print_tensor(quantized_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T22:00:03.658445Z",
     "start_time": "2024-12-15T22:00:03.625798Z"
    }
   },
   "id": "d29c3519c6aafa46"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(156, 512)\n    (wpe): Embedding(1024, 512)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-3): 4 x GPT2Block(\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=512, out_features=156, bias=False)\n)"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T16:39:34.768944Z",
     "start_time": "2024-12-16T16:39:34.721917Z"
    }
   },
   "id": "c58e8c372dd83abc"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Conv1D' has no attribute 'from_float'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m quantized_model \u001B[38;5;241m=\u001B[39m \u001B[43mquantize_gpt2_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[101], line 19\u001B[0m, in \u001B[0;36mquantize_gpt2_model\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mquantize_gpt2_model\u001B[39m(model):\n\u001B[0;32m---> 19\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mquantize_dynamic\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mqconfig_spec\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqconfig_spec\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqint8\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Specify the desired dtype\u001B[39;49;00m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Create a quantized copy\u001B[39;49;00m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:468\u001B[0m, in \u001B[0;36mquantize_dynamic\u001B[0;34m(model, qconfig_spec, dtype, mapping, inplace)\u001B[0m\n\u001B[1;32m    466\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m    467\u001B[0m propagate_qconfig_(model, qconfig_spec)\n\u001B[0;32m--> 468\u001B[0m \u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:553\u001B[0m, in \u001B[0;36mconvert\u001B[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inplace:\n\u001B[1;32m    552\u001B[0m     module \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(module)\n\u001B[0;32m--> 553\u001B[0m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remove_qconfig:\n\u001B[1;32m    557\u001B[0m     _remove_qconfig(module)\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    588\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    590\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[0;32m--> 591\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    592\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    588\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    590\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[0;32m--> 591\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    592\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "    \u001B[0;31m[... skipping similar frames: _convert at line 591 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    588\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    590\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[0;32m--> 591\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    592\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:593\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    590\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[1;32m    591\u001B[0m         _convert(mod, mapping, \u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# inplace\u001B[39;00m\n\u001B[1;32m    592\u001B[0m                  is_reference, convert_custom_config_dict)\n\u001B[0;32m--> 593\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m \u001B[43mswap_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_module_class_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    596\u001B[0m     module\u001B[38;5;241m.\u001B[39m_modules[key] \u001B[38;5;241m=\u001B[39m value\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:626\u001B[0m, in \u001B[0;36mswap_module\u001B[0;34m(mod, mapping, custom_module_class_mapping)\u001B[0m\n\u001B[1;32m    624\u001B[0m         new_mod \u001B[38;5;241m=\u001B[39m qmod\u001B[38;5;241m.\u001B[39mfrom_float(mod, weight_qparams)\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 626\u001B[0m         new_mod \u001B[38;5;241m=\u001B[39m \u001B[43mqmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_float\u001B[49m(mod)\n\u001B[1;32m    627\u001B[0m     swapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m swapped:\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: type object 'Conv1D' has no attribute 'from_float'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quantization import quantize_dynamic, default_dynamic_qconfig\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "# Define a custom quantization configuration\n",
    "qconfig_spec = {\n",
    "    nn.Linear: default_dynamic_qconfig,\n",
    "    Conv1D: default_dynamic_qconfig,  # Add Conv1D for GPT2\n",
    "}\n",
    "\n",
    "# Define a custom mapping for Conv1D to itself (dynamic quantization assumes the same layer works)\n",
    "from torch.quantization.quantization_mappings import get_default_dynamic_quant_module_mappings\n",
    "custom_mapping = get_default_dynamic_quant_module_mappings()\n",
    "custom_mapping[Conv1D] = Conv1D\n",
    "\n",
    "# Apply dynamic quantization\n",
    "def quantize_gpt2_model(model):\n",
    "    model = quantize_dynamic(\n",
    "        model,\n",
    "        qconfig_spec=qconfig_spec,\n",
    "        mapping=custom_mapping,\n",
    "        dtype=torch.qint8,  # Specify the desired dtype\n",
    "        inplace=False  # Create a quantized copy\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "quantized_model = quantize_gpt2_model(model.model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T22:21:10.248109Z",
     "start_time": "2024-12-15T22:21:10.025315Z"
    }
   },
   "id": "b1d7f09e91c0945d"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "sparse_state_dict = torch.load(\"/Users/sebastian/PycharmProjects/model-compression/models/rtf_small/id000017342868701547638784/rtf_model.pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T23:00:56.470061Z",
     "start_time": "2024-12-15T23:00:56.383756Z"
    }
   },
   "id": "6c8b27f7a53d19fa"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-15T23:01:41.390911Z",
     "start_time": "2024-12-15T23:01:41.198384Z"
    }
   },
   "id": "c3cd937dc942e209"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "603413fd1bafa07f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
