{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:20:58.716472Z",
     "start_time": "2025-01-30T20:20:49.812200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import src.data_processing as dp\n",
    "from src.rtf_pruning import prune_attention_heads,model_heads_by_magnitude\n",
    "from sdmetrics.reports.single_table import QualityReport\n",
    "from sdmetrics.reports.single_table import DiagnosticReport\n",
    "from realtabformer import REaLTabFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "train_data, test_data, sample_data = dp.csv_data_split(\"../data/breast-cancer-wisconsin.csv\")\n",
    "my_metadata_dict = dp.metadata(\"../data/cancer_metadata.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:25:56.404467Z",
     "start_time": "2025-01-30T20:25:56.382493Z"
    }
   },
   "id": "5adc58787ee41f20"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "rtf_model = REaLTabFormer.load_from_dir(\"../models/rtf_regular/id000017342890144858071040\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:25:58.340201Z",
     "start_time": "2025-01-30T20:25:57.118223Z"
    }
   },
   "id": "7c60d7c7d4d8332f"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 173740032 bytes\n"
     ]
    }
   ],
   "source": [
    "# Size of model before pruning attn heads\n",
    "def model_size(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "print(f\"Model size: {model_size(rtf_model.model)} bytes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:25:58.976472Z",
     "start_time": "2025-01-30T20:25:58.960304Z"
    }
   },
   "id": "abd5a1c0aa629510"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 has 12 attention heads.\n",
      "Layer 1 has 12 attention heads.\n",
      "Layer 2 has 12 attention heads.\n",
      "Layer 3 has 12 attention heads.\n",
      "Layer 4 has 12 attention heads.\n",
      "Layer 5 has 12 attention heads.\n"
     ]
    }
   ],
   "source": [
    "# Prints how many attentions heads before pruning \n",
    "for i, block in enumerate(rtf_model.model.transformer.h):\n",
    "    num_heads = block.attn.num_heads\n",
    "    print(f\"Layer {i} has {num_heads} attention heads.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:25:59.784311Z",
     "start_time": "2025-01-30T20:25:59.761517Z"
    }
   },
   "id": "c8672626fc5ad821"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/realtabformer/realtabformer.py:77: UserWarning: The device=cuda is not available, using device=cpu instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/137 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94452a2fa39149a4ba45e91d0ebfd573"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0 invalid samples out of total 256 samples generated. Sampling efficiency is: 100.0000%\n",
      "             Property     Score\n",
      "0       Column Shapes  0.923358\n",
      "1  Column Pair Trends  0.910994\n",
      "         Property     Score\n",
      "0   Data Validity  0.992701\n",
      "1  Data Structure  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Quality report before pruning\n",
    "synthetic_data = rtf_model.sample(n_samples=len(test_data))\n",
    "\n",
    "quality = QualityReport()\n",
    "diagnostic = DiagnosticReport()\n",
    "\n",
    "quality.generate(test_data,synthetic_data,metadata=my_metadata_dict,verbose=False)\n",
    "diagnostic.generate(test_data,synthetic_data,metadata=my_metadata_dict,verbose=False)\n",
    "\n",
    "print(quality.get_properties())\n",
    "print(diagnostic.get_properties())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:26:36.669364Z",
     "start_time": "2025-01-30T20:26:26.262116Z"
    }
   },
   "id": "388dbf1df86cf0c7"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, [8, 2, 1, 7]),\n (1, [2, 11, 0, 7]),\n (2, [5, 6, 8, 0]),\n (3, [5, 2, 10, 9]),\n (4, [8, 2, 3, 6]),\n (5, [6, 4, 5, 0])]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding least to most important attention heads per layer\n",
    "heads_to_prune = model_heads_by_magnitude(model=rtf_model.model,percentage=0.3,num_heads_per_layer=12,layers=6)\n",
    "heads_to_prune"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:26:36.677574Z",
     "start_time": "2025-01-30T20:26:36.660452Z"
    }
   },
   "id": "5345f8a43349b66f"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "prune_attention_heads(rtf_model.model,heads_to_prune)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:26:36.933083Z",
     "start_time": "2025-01-30T20:26:36.678658Z"
    }
   },
   "id": "b6be1d08fd92407d"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 has 8 attention heads.\n",
      "Layer 1 has 8 attention heads.\n",
      "Layer 2 has 8 attention heads.\n",
      "Layer 3 has 8 attention heads.\n",
      "Layer 4 has 8 attention heads.\n",
      "Layer 5 has 8 attention heads.\n"
     ]
    }
   ],
   "source": [
    "# After pruning \n",
    "for i, block in enumerate(rtf_model.model.transformer.h):\n",
    "    num_heads = block.attn.num_heads\n",
    "    print(f\"Layer {i} has {num_heads} attention heads.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:26:36.953880Z",
     "start_time": "2025-01-30T20:26:36.938731Z"
    }
   },
   "id": "204809056447f59"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 154847232 bytes\n"
     ]
    }
   ],
   "source": [
    "# Size of model after pruning attn heads\n",
    "def model_size(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "print(f\"Model size: {model_size(rtf_model.model)} bytes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:26:54.379767Z",
     "start_time": "2025-01-30T20:26:54.355605Z"
    }
   },
   "id": "bee0f21dd6822334"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/realtabformer/realtabformer.py:77: UserWarning: The device=cuda is not available, using device=cpu instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/137 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a43d436aea7a46d4b51faed79df9fbcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0 invalid samples out of total 256 samples generated. Sampling efficiency is: 100.0000%\n",
      "             Property     Score\n",
      "0       Column Shapes  0.870073\n",
      "1  Column Pair Trends  0.881952\n",
      "         Property     Score\n",
      "0   Data Validity  0.993364\n",
      "1  Data Structure  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Evaluation after pruning attn heads\n",
    "synthetic_data_2 = rtf_model.sample(n_samples=(len(test_data)))\n",
    "\n",
    "quality = QualityReport()\n",
    "diagnostic = DiagnosticReport()\n",
    "\n",
    "quality.generate(test_data,synthetic_data_2,metadata=my_metadata_dict,verbose=False)\n",
    "diagnostic.generate(test_data,synthetic_data_2,metadata=my_metadata_dict,verbose=False)\n",
    "\n",
    "print(quality.get_properties())\n",
    "print(diagnostic.get_properties())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:27:45.176378Z",
     "start_time": "2025-01-30T20:27:31.842601Z"
    }
   },
   "id": "753e360eb493f43"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying artefacts from: best-disc-model\n",
      "Copying artefacts from: mean-best-disc-model\n",
      "Copying artefacts from: not-best-disc-model\n",
      "Copying artefacts from: last-epoch-model\n"
     ]
    }
   ],
   "source": [
    "rtf_model.save(\"../models/saved/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:32:47.927561Z",
     "start_time": "2025-01-30T20:32:47.174977Z"
    }
   },
   "id": "aeb9096c4fc1526a"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.h.0.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.0.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.0.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.1.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.1.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.1.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.2.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.2.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.2.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.3.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.3.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.3.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.4.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.4.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.4.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.5.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.5.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.5.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Since size of saved model and expected model don't match up after pruning loading doesn't work, \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# the config needs to be updated after pruning or making any adjustments to the model\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43mREaLTabFormer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../models/saved/id000017342890144858071040\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/realtabformer/realtabformer.py:1572\u001B[0m, in \u001B[0;36mREaLTabFormer.load_from_dir\u001B[0;34m(cls, path)\u001B[0m\n\u001B[1;32m   1569\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1570\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid model_type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrealtf\u001B[38;5;241m.\u001B[39mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1572\u001B[0m \u001B[43mrealtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1573\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_posix\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m   1574\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1576\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m realtf\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2148\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2149\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2150\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2154\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.h.0.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.0.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.0.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.1.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.1.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.1.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.2.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.2.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.2.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.3.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.3.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.3.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.4.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.4.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.4.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for transformer.h.5.attn.c_attn.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n\tsize mismatch for transformer.h.5.attn.c_attn.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for transformer.h.5.attn.c_proj.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([768, 768])."
     ]
    }
   ],
   "source": [
    "# Since size of saved model and expected model don't match up after pruning loading doesn't work, \n",
    "# the config needs to be updated after pruning or making any adjustments to the model\n",
    "test = REaLTabFormer.load_from_dir(\"../models/saved/id000017342890144858071040\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T20:33:13.918613Z",
     "start_time": "2025-01-30T20:33:11.744558Z"
    }
   },
   "id": "2d661bba7696f87d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
