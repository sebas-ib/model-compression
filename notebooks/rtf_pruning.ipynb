{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T19:05:58.452333Z",
     "start_time": "2024-12-16T19:05:50.310265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers.pytorch_utils import Conv1D,prune_conv1d_layer,find_pruneable_heads_and_indices\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import transformers.pytorch_utils\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import src.data_processing as dp\n",
    "from sdmetrics.reports.single_table import QualityReport\n",
    "from sdmetrics.reports.single_table import DiagnosticReport\n",
    "from realtabformer import REaLTabFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# def prune_conv1d(layer: Conv1D, index: torch.LongTensor, dim: int = 1) -> Conv1D:\n",
    "#     index = index.to(layer.weight.device)  # Ensure the index tensor is on the same device as the layer\n",
    "#     W = layer.weight.index_select(dim, index).clone().detach()  # Prune weights along the specified dimension\n",
    "# \n",
    "#     # Adjust bias if present, ensuring it matches the pruned weight size\n",
    "#     if layer.bias is not None:\n",
    "#         b = layer.bias[index].clone().detach()\n",
    "#     else:\n",
    "#         b = None  # No bias in the layer\n",
    "# \n",
    "#     # Calculate new size after pruning\n",
    "#     new_size = list(layer.weight.size())\n",
    "#     new_size[dim] = len(index)  # Adjust the pruned dimension to match the selected indices\n",
    "# \n",
    "#     # Create a new Conv1D layer with the pruned size\n",
    "#     new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
    "#     new_layer.weight.requires_grad = False\n",
    "#     new_layer.weight.copy_(W.contiguous())\n",
    "#     new_layer.weight.requires_grad = True\n",
    "# \n",
    "#     # If there's a bias, copy it as well\n",
    "#     if b is not None:\n",
    "#         new_layer.bias.requires_grad = False\n",
    "#         new_layer.bias.copy_(b.contiguous())\n",
    "#         new_layer.bias.requires_grad = True\n",
    "# \n",
    "#     return new_layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "922766be067efe55"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\n",
    "def prune_heads_custom(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n",
    "        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
    "\n",
    "        # Prune conv1d layers\n",
    "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n",
    "        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n",
    "\n",
    "        # Update hyper params\n",
    "        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n",
    "        self.num_heads = self.num_heads - len(heads)\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "\n",
    "def apply_structured_pruning(model):\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, Conv1D):\n",
    "            pruned_layers.append(name)\n",
    "    \n",
    "    for name in pruned_layers:\n",
    "        if 'transformer.h.' in name:\n",
    "            block_index = int(name.split('.')[2])\n",
    "            \n",
    "            block = model.transformer.h[block_index]\n",
    "            \n",
    "            if 'attn' in name:\n",
    "                test = [0,1,2,3]\n",
    "                prune_heads_custom(block.attn,test)\n",
    "            # elif 'mlp' in name:\n",
    "            #     if 'c_fc' in name:\n",
    "            #         block.mlp.c_fc = pruned_layer\n",
    "            #     elif 'c_proj' in name:\n",
    "            #         block.mlp.c_proj = pruned_layer\n",
    "        \n",
    "        # print(f\"Replaced layer {name} with pruned weight shape: {pruned_layer.weight.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:14:29.534380Z",
     "start_time": "2024-12-16T20:14:29.522242Z"
    }
   },
   "id": "cdf6740ea08a8257"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def compute_attention_scores(model, dataset):\n",
    "    # Example method to calculate attention scores\n",
    "    attention_scores = {i: [] for i in range(model.num_heads)}\n",
    "    \n",
    "    for input_data in dataset:\n",
    "        attention_weights = model.get_attention_weights(input_data)  # Retrieve attention weights\n",
    "        for head_idx in range(model.num_heads):\n",
    "            attention_scores[head_idx].append(torch.mean(attention_weights[head_idx]).item())\n",
    "    \n",
    "    return attention_scores\n",
    "\n",
    "def print_tensor(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() == 2:\n",
    "            print(name,param.size())\n",
    "\n",
    "def convert_to_sparse(model, pruned_layers):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the layer is in the pruned list\n",
    "        if any(layer_name in name for layer_name in pruned_layers):\n",
    "            if param.dim() == 2:  # Sparse only weight matrices\n",
    "                sparse_param = param.to_sparse()\n",
    "                param.data = sparse_param\n",
    "\n",
    "def check_layer_names(model):\n",
    "    for name, module in model.named_modules():\n",
    "        print(f\"Layer Name: {name}, Module: {module.__class__.__name__}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T19:21:50.361593Z",
     "start_time": "2024-12-16T19:21:50.347819Z"
    }
   },
   "id": "10e18b69bc0404bd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data, test_data, sample_data = dp.csv_data_split(\"../data/breast-cancer-wisconsin.csv\")\n",
    "my_metadata_dict = dp.metadata(\"../data/cancer_metadata.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T19:21:53.758935Z",
     "start_time": "2024-12-16T19:21:53.735344Z"
    }
   },
   "id": "5adc58787ee41f20"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "rtf_model = REaLTabFormer.load_from_dir(\"../models/rtf_small_copy/id000017342868701547638784\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:13:39.490780Z",
     "start_time": "2024-12-16T20:13:38.835149Z"
    }
   },
   "id": "7c60d7c7d4d8332f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'num_heads'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcompute_attention_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrtf_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m, in \u001B[0;36mcompute_attention_scores\u001B[0;34m(model, dataset)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_attention_scores\u001B[39m(model, dataset):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Example method to calculate attention scores\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m {i: [] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m)}\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m input_data \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[1;32m      6\u001B[0m         attention_weights \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_attention_weights(input_data)  \u001B[38;5;66;03m# Retrieve attention weights\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1687\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1688\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'GPT2LMHeadModel' object has no attribute 'num_heads'"
     ]
    }
   ],
   "source": [
    "compute_attention_scores(rtf_model.model,train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T19:22:07.682099Z",
     "start_time": "2024-12-16T19:22:06.822934Z"
    }
   },
   "id": "330c8d2ca604a2ed"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 has GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") attention heads.\n",
      "Layer 1 has GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") attention heads.\n",
      "Layer 2 has GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") attention heads.\n",
      "Layer 3 has GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") attention heads.\n"
     ]
    }
   ],
   "source": [
    "for i, block in enumerate(rtf_model.model.transformer.h):\n",
    "    num_heads = block.mlp\n",
    "    print(f\"Layer {i} has {num_heads} attention heads.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:13:43.625019Z",
     "start_time": "2024-12-16T20:13:43.593386Z"
    }
   },
   "id": "204809056447f59"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/realtabformer/realtabformer.py:77: UserWarning: The device=cuda is not available, using device=cpu instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/137 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58dd83b8f0bc4b45834a8db2f971429d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0 invalid samples out of total 256 samples generated. Sampling efficiency is: 100.0000%\n",
      "             Property     Score\n",
      "0       Column Shapes  0.937226\n",
      "1  Column Pair Trends  0.916875\n",
      "         Property     Score\n",
      "0   Data Validity  0.998673\n",
      "1  Data Structure  1.000000\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = rtf_model.sample(n_samples=len(test_data))\n",
    "\n",
    "quality = QualityReport()\n",
    "diagnostic = DiagnosticReport()\n",
    "\n",
    "quality.generate(test_data,synthetic_data,metadata=my_metadata_dict,verbose=False)\n",
    "diagnostic.generate(test_data,synthetic_data,metadata=my_metadata_dict,verbose=False)\n",
    "\n",
    "print(quality.get_properties())\n",
    "print(diagnostic.get_properties())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:13:51.454627Z",
     "start_time": "2024-12-16T20:13:46.701027Z"
    }
   },
   "id": "25f918893e032fc5"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([156, 512])\n",
      "transformer.wpe.weight torch.Size([1024, 512])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([512, 768])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([256, 512])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([512, 2048])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([2048, 512])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([512, 768])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([256, 512])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([512, 2048])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([2048, 512])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([512, 768])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([256, 512])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([512, 2048])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([2048, 512])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([512, 768])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([256, 512])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([512, 2048])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([2048, 512])\n"
     ]
    }
   ],
   "source": [
    "print_tensor(rtf_model.model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:14:39.980083Z",
     "start_time": "2024-12-16T20:14:39.958933Z"
    }
   },
   "id": "80a76f2f29e3498f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3fcee49369f14a"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# indices = torch.LongTensor([i for i in range(0, 512, 2 )])  # Keep 100 out of 512 channels\n",
    "apply_structured_pruning(rtf_model.model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:14:35.233966Z",
     "start_time": "2024-12-16T20:14:35.157907Z"
    }
   },
   "id": "cc6785d8da856e47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rtf_model.model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9f07a9f40f37a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.00%\n",
      "Total: 11114496\n",
      "Zero: 0\n"
     ]
    }
   ],
   "source": [
    "def compute_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "        zero_params += (param == 0).sum().item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity,total_params, zero_params\n",
    "\n",
    "# Example usage\n",
    "sparsity, total_params, zero_params = compute_sparsity(rtf_model.model)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n",
    "print(f\"Total: {total_params}\")\n",
    "print(f\"Zero: {zero_params}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:14:57.929420Z",
     "start_time": "2024-12-16T20:14:57.885366Z"
    }
   },
   "id": "60906374c2fd3d36"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8410693529639675"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11114496/13214720"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:15:04.683347Z",
     "start_time": "2024-12-16T20:15:04.663823Z"
    }
   },
   "id": "edd143136ed50313"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 44457984 bytes\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    # Compute the total size of the model in bytes\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "print(f\"Original model size: {model_size(rtf_model.model)} bytes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:15:09.762661Z",
     "start_time": "2024-12-16T20:15:09.742641Z"
    }
   },
   "id": "cfdcf41e76fd3d37"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/PycharmProjects/model-compression/venv/lib/python3.11/site-packages/realtabformer/realtabformer.py:77: UserWarning: The device=cuda is not available, using device=cpu instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/137 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b9f7ecd20c242efbcd5f83344796f02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0 invalid samples out of total 256 samples generated. Sampling efficiency is: 100.0000%\n",
      "             Property     Score\n",
      "0       Column Shapes  0.891971\n",
      "1  Column Pair Trends  0.803713\n",
      "         Property     Score\n",
      "0   Data Validity  0.998673\n",
      "1  Data Structure  1.000000\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = rtf_model.sample(n_samples=(len(test_data)))\n",
    "\n",
    "quality = QualityReport()\n",
    "diagnostic = DiagnosticReport()\n",
    "\n",
    "quality.generate(test_data,synthetic_data,metadata=my_metadata_dict,verbose=False)\n",
    "diagnostic.generate(test_data,synthetic_data,metadata=my_metadata_dict,verbose=False)\n",
    "\n",
    "print(quality.get_properties())\n",
    "print(diagnostic.get_properties())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T20:15:40.322246Z",
     "start_time": "2024-12-16T20:15:35.477694Z"
    }
   },
   "id": "753e360eb493f43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
